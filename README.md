# Data-Science-Projects
Collection of data science projects, ranging between personal and educational and research!

[Iris Species Prediction](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Iris%20Species%20Prediction): Tested the accuracy of different machine learning algorithms in predicting the exact species of *Iris* flower based off *Iris* flower data set collected by Edgar Anderson to study the morphologic variation within closely related species of *Iris* flowers. Upon deciding to use the K-Nearest Neighbors algorithm for its accuracy and simplicity, tested the accuracy of my model on the validation set and quantified the model's accuracy for predicting the species of *Iris* based on sepal and petal measurements to be 90%.

***********************************************************

[Loan Prediction](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Loan%20Prediction): Automated processes of a theoretical home loan company by predicting whether or not an individual's loan would be approved by using classification algorithms. Finally settled on using the Random forest algorithm with the predictor variables as follows: Total Income, Loan Amount, Credit History, Dependents, and Property Area. My model, using a Random Forest algorithm, yielded an accuracy of 82.736% and a cross-validation score of 80.297%. 

***********************************************************

[Wine Quality Prediction](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Wine%20Quality%20Prediction): Utilized principal component analysis (PCA) to reduce the variables in a wine quality data set, from this [link](https://archive.ics.uci.edu/ml/datasets/Wine+Quality). After reducing variables to 6 latent variables decided via the aforementioned PCA, used a Na√Øve Bayes classifier to predict the quality of wine on a test subset of the overall dataset based upon these 6 latent variables. The accuracy I achieved with this model was a relatively low 53.873%. This can be either attributed to the model being inaccurate, or to the fact that wine quality is based arbitrarily on critics' opinions and can't be consistently predicted through objective qualities of the wine. 

***********************************************************

[Boston Home Price Prediction](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Boston%20Home%20Price%20Prediction): Given sample data about 506 homes in Boston and 13 features for each sample, built a linear regression model to predict the price of a house. First, performed regression with only one feature, which in this case was the feature that indicated the number of rooms in a house. The Root Mean Squared Error and R^2 for this regression were 4.9 and .69, respectively. Next, performed a regression using all features, resulting in a Root Mean Squared Error and R^2 of 4.9 and .67, respectively, indicating some benefits of using a regression with just one feature, as this regerssion more accurately fits the data. 

***********************************************************

[BBC Article Classification](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/BBC%20Article%20Classification): Downloaded publicly available [BBC news article datasets](http://mlg.ucd.ie/datasets/bbc.html) with the intention of deciding on a supervised learning algorithm that can most accurately categorize articles into either 'sport', 'business', 'tech', 'politics', or 'entertainment' when provided with the text of the article, along with gaining valuable insight into factors that differentiate news articles in these categories. 

First, created a dataset with the following features: 'Filename', 'Content', 'Category' through readtext package in [RStudio](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/BBC%20Article%20Classification/dataset_creation.R). 'Filename' is the name of the file that contains the news article being observed in that row, 'Content' is the un-parsed text of the news article, and 'Category' is which news category the article belongs to. 

Next, performed [exploratory data analysis and data cleansing](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/BBC%20Article%20Classification/EDA_bbc_article.ipynb) by removing "README.TXT" instances, observing percentage distribution of articles by category, as well as distribution of articles by article length, while making adjustments to dataset to bring distributions closer to a normal distribution. 

Most recent action taken on this project is the [feature engineering](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/BBC%20Article%20Classification/bbc-articles-featureengineering.ipynb), where the following parsing of the content was done: delete special characters, lowercase all characters, delete punctuation marks, ignore possessive cases, lemmatize the content, and delete English stopwords. Furthermore, category codes were assinged to each different category of news article type and train/test datasets were formed. Finally, elected to use TF-IDF vector representation for text and set the appropriate parameters for the algorithm. 

***********************************************************

[blink-182 Discography Comparison](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/blink-182%20Discography%20Comparison): Used the [Spotify Web API](https://developer.spotify.com/documentation/web-api/) to access music and discography data for American rock band [blink-182](https://en.wikipedia.org/wiki/Blink-182). Afterwards, I cleaned up the dataset and removed some features, and then used Euclidean distance to cluster the individual songs within an album ([dendrogram for *NINE*](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/blink-182%20Discography%20Comparison/Rplot%20-%20nine.jpeg) linked as an example), the [individual songs within blink-182's entire discography](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/blink-182%20Discography%20Comparison/album%20overall%20song%20comparison.jpeg), and each [overall album to the other albums](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/blink-182%20Discography%20Comparison/album%20overall%20comparison%20average.jpeg). The clustering observed in the data can be correlated to changes in the band line-up for each cluster of albums as well as stylistic shifts in the music played by the band within that specific album. Finally, the code used to compare the individual blink-182 songs within its respective album can be found [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/blink-182%20Discography%20Comparison/blink-182%20album%20individual%20comparison.R), as well as the code for comparing their individual songs to their entire discography and their overall albums to each other [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/blink-182%20Discography%20Comparison/blink-182%20album%20overall%20comparison.R)

***********************************************************

[Comparing 50 Years of Billboard Hot 100](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Comparing%2050%20Years%20of%20Billboard%20Hot%20100): Downloaded [dataset](https://www.kaggle.com/rakannimer/billboard-lyrics) that contains features from Billboard's Year-End Hot 100 ranging from 1965 to 2015. Features in this original set included are the year a song appeared on the Year-End Hot 100, artist, song name, and lyrics. I then used the [Spotify Web Api](https://developer.spotify.com/documentation/web-api/) and the R library [spotifyr](https://github.com/charlie86/spotifyr) to add audio features (danceability, energy, key, etc.) relating to each artist's song that was in the Year-End Hot 100 dataset. After achieving this dataframe, many values had to be cleaned out, owing to inadequate logical comparisons that resulted in certain artists or songs not being recognized by spotifyr's request for artist audio features. Nevertheless, the Euclidean distance between the songs was calculated, and the songs were grouped together by the averages of the Euclidean distance by the year the song was on the Billboard's Year-End Hot 100. Finally, the averages were clustered together and visualized through a [dendrogram](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Comparing%2050%20Years%20of%20Billboard%20Hot%20100/billboard_dend_1.pdf). Future attempts to improve this project would revolve around 1) attempting to quicken the process of adding audio features (the for-loop I implemented took a few hours to complete), 2) trying to minimize songs lost through inadequate logical comparisons, and 3) implementing more intuitive visualizations to better understand how popular music has changed over 50 years. 

***********************************************************

[Quora Data Challenge](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Quora%20Data%20Challenge): Contains original files provided for Quora Data Challenge and the Python and R code I used to conduct T-tests and calculate the 95% confidence intervals. Overall project was essentially hypothesis testing to decide whether or not a newly designed UI for an app should be pushed to production based on differnces in the active minutes logged by a treatment group and a control group after the treatment group was given the new UI to use. Through T-tests of different subsets of the samples, I reached the conclusion that the new UI for the app should be pushed to production. 

***********************************************************

[Haplotype Inference](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Haplotype%20Inference): Given masked genotype in the form of a matrix of x single nucleotide polymorphisms, or SNPs, and y individuals , with random, unknown values, inferred the haplotypes for each individual at each SNP using Python 3. First, the masked genotype was [unmasked](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/unmask_genotype.py), then the unmasked genotype was used to construct a [haplotype matrix](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/produce_haplotype_matrix.py) of x SNPs and y x 2 haplotypes, given an unmasked genotype matrix of x SNPS and y genotypes. The [driver code](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/driver_code.py) combines the functions to produce two haplotype matrices for example genotype data, and the haplotype matrix for a test genotype data. Furthermore, a [function](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/haplotype_accuracy.R) to calculate the percent accuracy of this algorithm when comparing the reached haplotype to the actual haplotype was implemented in R. As far as results go, two haplotypes inferred from this process were 85.5% and 84.9% accurate when compared to the actual haplotypes. The inferred haplotypes are linked [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/ex1_result.txt) and [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/ex2_result.txt), while the actual haplotypes are linked [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/example_data_1_sol.txt) and [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Haplotype%20Inference/example_data_2_sol.txt). 

***********************************************************

[Highest Correlated Single Nucleotide Polymorphism](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Highest%20Correlated%20Single%20Nucleotide%20Polymorphism): [Given](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Highest%20Correlated%20Single%20Nucleotide%20Polymorphism/PhenotypeGenotype100SNP50N.csv) the phenotypes of 50 different individuals, as well as readings from 100 single nucleotide polymorphisms, or SNPs, found the SNPs that have the highest correlation with all of the phenotypes. The problem was solved by fitting a [linear regression model](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/Highest%20Correlated%20Single%20Nucleotide%20Polymorphism/snp_phenotype_correlation.R) to the data in R, finding the correlation coefficients for each SNP, and then parsing these values to find the SNPs with the highest correlation values for all the observed phenotypes. 

***********************************************************

[PCA Clustering Based on Genotype](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/PCA%20Clustering%20Based%20on%20Genotype): [Provided](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/PCA%20Clustering%20Based%20on%20Genotype/Genotype40kSNP50N.txt) the genotypes of fifty individuals in the form of readings at about 40,000 single nucleotide polymorphisms, or SNPs, [performed](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/PCA%20Clustering%20Based%20on%20Genotype/genotype_pca_kmeans_cluster.R) a principal component analysis and used k-means clustering to segment the data across the first two principal components. Most of the variance in the data could be explained along the first principal component, as seen [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/PCA%20Clustering%20Based%20on%20Genotype/PCA-Variance.png), which supports the fact that the clustering visually aligns with how the first principal component splits the data, as seen [here](https://github.com/ankushbharadwaj/Data-Science-Projects/blob/master/PCA%20Clustering%20Based%20on%20Genotype/PCA-Clustered.png). 

***********************************************************

[Cryptocurrency Analysis and Prediction](https://github.com/ankushbharadwaj/Data-Science-Projects/tree/master/Cryptocurrency%20Analysis%20and%20Prediction): The goal of this project was to decide a course of action to maximize monetary gains for a given cryptocurrency and its opening value, number of trades, volume, highest value, and lowest value. To answer this question, I used the Binance API to put together a dataset of various cryptocurrencies at different time intervals and many attributes of these cryptocurrencies at their respective time intervals. Afterwards, I prepared the data by ignoring unessential attributes, and I ensured that all the columns were of a proper data type. Next, I performed exploratory data analysis to understand how the mean difference between opening and closing values varied between different cryptocurrencies on different time intervals. Finally, I fitted a linear regression model to the data to predict the closing value of the given cryptocurrency, which was then used to justify my recommendation to sell the cryptocurrency before the end of the day, as its predicted closing value is greater than the opening value.

***********************************************************

